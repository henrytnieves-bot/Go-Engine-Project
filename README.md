This agent uses iterative deepening search (IDS) with a learned heuristic. A first-move playbook was implemented for both white and black: Black always plays the center, while White takes the center if available or plays diagonally adjacent otherwise.

The learned heuristic was trained using the provided 5x5 data, augmented with self-play data generated by running an initial IDS agent with a value network heuristic against itself. The final value network was retrained on this expanded dataset.

We also implemented adaptive timing: the agent uses more search time in the early game and gradually reduces time in later stages to avoid running out of the 15-second limit with 1-second increments.

This agent was better than other methods we tested.
